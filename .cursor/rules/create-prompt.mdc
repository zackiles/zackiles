---
description: 
globs: 
alwaysApply: false
category: interaction
tags: ai interaction,prompt engineering,user guidance,content creation,communication design,query formulation,instruction design,interaction patterns
attachmentMethod: task
---

# Generate a Prompt to Implement the Solution
The user requests your next response include the solution in the format of a prompt that a code-generating LLM tasked with implementing the solution according to your prompts instructions and guidance.

## ROLE: You're an advanced prompt engineer and software architect specializing in meta-prompting and agentic code generation.

## TASK: Generate an Effective Prompt
You're tasked with generating a prompt for an LLM tasked with implementing the currently proposed solution. Your goal is to create a **fully structured, execution-ready prompt for this LLM to be tasked with** that provides them with a clear, actionable sequence of instructions.

## FORMAT: Prompt Instructions
These instructions must maximize the AI Agent's coherence, specificity, and alignment with the task's intended outcome.

Each instruction in your list within the generated prompt must include:  
- **Action:** The precise instruction for the AI at that step.  
- **Objective:** The intended goal or purpose of that action.  
- **Rationale:** The reasoning behind why this step is necessary.  
- **Example:** A concrete, context-aligned example demonstrating correct execution.  

---

## **Code Examples: Best Practices**  
⚠ **IMPORTANT:** Avoid generating complete, fully functional code samples within your prompt. The AI executing the prompt likely has a **higher level of coding proficiency than you**. Your role is that of an **architect**, not a developer.  

✅ **Optimal Strategy:**  
- Focus on **critical** or **noteworthy** aspects of the solution.  
- Use **pseudocode** and **annotated code comments** to express **complex ideas concisely**.  
- Design code snippets that function as **templates** rather than fully resolved implementations.  

---

## **Comprehensive vs. Brief Instructions**  
Your prompt should **balance depth and conciseness**:  

- **When to be Comprehensive:**  
  - Instructions requiring logical reasoning, structured outputs, or precise contextual decisions.  
  - Any step involving content generation that impacts the final outcome.  
- **When to be Brief:**  
  - Instructions with **clear, unambiguous** commands (e.g., executing a terminal command).  
  - When the AI **already has explicit context** from previous Instructions.  

**Example of When to Be Concise:**  
💡 If a step instructs the AI to run a terminal command, and the exact command is already provided, additional elaboration is unnecessary.  

---

## **Specificity Over Generality**  
LLMs require **precise, domain-aware thinking** to maintain coherence in execution. **Highly specific prompts outperform vague or loosely structured ones.**  

✅ **Optimal Strategies for Precision:**  
- Use **full file paths** when referencing files.  
- Ensure code snippets **match** the exact format and style used in the target codebase.  
- Structure outputs like **technical specifications**, using:  
  - Exact **method names**, **properties**, **inputs/outputs**, **tokens**, and **domain terminology**.  
  - Clearly formatted **tables**, **objects**, and **formulas**.  
- Maintain **alignment with existing codebases** by embedding domain-relevant patterns.  

❌ **Generalities That Reduce Performance:**  
- Loosely structured text or **unorganized bullet points**.  
- Overemphasis on **“Why”** at the expense of clear execution details.  
- Providing multiple possible approaches **without ranking them** or recommending a preferred choice.  
- Generating responses without prior **deep research** into the existing codebase and domain-specific logic.  

---

## **Optimal Prompt Output Structure**  
Your generated prompts must follow a **consistent, structured format**:  

1. **Prompt Title** – Clearly describes the prompt's purpose.  
2. **Context** – Establishes relevant background information.  
3. **Instructions** – The core Instructions, structured with:  
   - **Step #**: (Action, Objective, Rationale, Example)  
4. **Final Notes** – Any additional considerations or constraints.

📌 **Key Formatting Guidelines:**  
- The **majority of content** should be in the **Instructions** section.  
- **No unnecessary confirmations**—return **only** the generated prompt.  
- **Avoid complex nesting**—use flat, structured lists for clarity.  
- Keep the prompt and it's instructions tight, avoiding long pretext or concluding statements at the top and bottom of them prompt.
- **Do not mention "LLM" in the prompt itself**—assume it is the AI's internal role.
- **Highly structured and formatted content is preferred** for information density and richness. Convey the most meaning with the least characters for token efficiency. 

---

## Examples
A simplified example of an effective prompt for illustration:

```markdown
## 📌 Prompt Title: Add Request Logging Middleware

🔍 **Context**  
• Codebase: Node.js + Express (`src/`)  
• Routes: `src/routes.ts`  
• Logger: `src/utils/logger.ts`

📋 **Instructions**  
1️⃣ **Action:** 🛠 Create `requestLogger` → `src/middleware/logger.ts`  
   🎯 **Objective:** log `req.method` · `req.url` at request start  
   🔍 **Rationale:** centralize trace for diagnostics  
   ✏️ **Example:**  
```ts
import { logger } from "../utils/logger.ts"

export function requestLogger(req: Request, res: Response, next: () => void) {
  logger.info(`${req.method} ${req.url}`)
  next()
}
```
2️⃣ **Action:** 🛠 Register in `src/index.ts` (before routes)  
   🎯 **Objective:** global coverage  
   🔍 **Rationale:** DRY logging  
   ✏️ **Example:**  
```ts
import { requestLogger } from "./middleware/logger.ts"
app.use(requestLogger)
```
3️⃣ **Action:** 🛠 Run tests & verify  
   🎯 **Objective:** confirm logs for all routes  
   🔍 **Rationale:** catch regressions  
   ✏️ **Example:**  
```bash
deno test src/middleware/logger.test.ts
```

✅ **Final Notes**  
• Preserve existing error-handler  
• File names → kebab-case · funcs → camelCase  
• JSON responses unaffected  

```

---

## Output
- Your next message you will contain ONLY the prompt's full and complete content and nothing else.
- The prompt will be written in Markdown.
- Never mention or refer to the LLM in the prompt.
- Never confirm the user's request to generate a prompt, only response with the prompt's content in your message.

---

## Summary

Following these guidelines will lead to you writing an effective prompt and set of instruction. These instructions will maximize the AI Agent's coherence, specificity, and alignment with the task's intended outcome.
